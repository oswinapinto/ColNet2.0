{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output\n",
    "from pattern.text.en import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from util_cnn import generate_synthetic_columns, synthetic_columns2sequence, sequence2matrix, random_cells2synthetic_columns, ordered_cells2synthetic_columns, permutation_cells2synthetic_columns\n",
    "from lookup import WikidataAPI\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--model_dir',\n",
    "    type=str,\n",
    "    default='../../enwiki_model/',\n",
    "    help='Directory of word2vec model')\n",
    "parser.add_argument(\n",
    "    '--synthetic_column_size',\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help='Size of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--synthetic_column_type',\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help='synthetic column num to sample for each column; '\n",
    "         '>=1: sample a number; 0: sliding window; -1: permutation combination and voting')\n",
    "parser.add_argument(\n",
    "    '--sequence_size',\n",
    "    type=int,\n",
    "    default=60,\n",
    "    help='Length of word sequence of entity unit')\n",
    "parser.add_argument(\n",
    "    '--cnn_evaluate',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'output/cnn/cnn_1_2_1.00'),\n",
    "    help='Directory of trained models')\n",
    "parser.add_argument(\n",
    "    '--output_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'output/'),\n",
    "    help='Directory of output')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "prediction_dir = os.path.join(FLAGS.output_dir,'predictions')\n",
    "if not os.path.exists(prediction_dir):\n",
    "    os.mkdir(prediction_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load word2vec model ...\n"
     ]
    }
   ],
   "source": [
    "print('load word2vec model ...')\n",
    "w2v_model = Word2Vec.load(os.path.join(FLAGS.model_dir, 'word2vec_gensim'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open(file) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "\n",
    "def load_model(cnn_model_directory, candidate_class):\n",
    "    return keras.models.load_model(cnn_model_directory+'\\%s' % candidate_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_samples(pos, neg):\n",
    "    if len(pos) <= len(neg):\n",
    "        pos_new = pos * int(len(neg) / len(pos))\n",
    "        neg_new = neg * 1\n",
    "        pos_new += random.sample(pos, len(neg_new) - len(pos_new))\n",
    "    else:\n",
    "        neg_new = neg * (len(pos) / len(neg))\n",
    "        pos_new = pos * 1\n",
    "        neg_new += random.sample(neg, len(pos_new) - len(neg_new))\n",
    "    return pos_new, neg_new\n",
    "\n",
    "\n",
    "def embedding(entities_positive, entities_negative):\n",
    "    # embedding\n",
    "    units_positive = generate_synthetic_columns(\n",
    "        entities_positive, FLAGS.synthetic_column_size)\n",
    "    units_negative = generate_synthetic_columns(\n",
    "        entities_negative, FLAGS.synthetic_column_size)\n",
    "\n",
    "    sequences_positive = list()\n",
    "    for ent_unit in units_positive:\n",
    "        sequences_positive.append(\n",
    "            synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "    sequences_negative = list()\n",
    "    for ent_unit in units_negative:\n",
    "        sequences_negative.append(\n",
    "            synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "\n",
    "    x = np.zeros((len(sequences_positive) + len(sequences_negative),\n",
    "                 FLAGS.sequence_size, w2v_model.vector_size, 1))\n",
    "    for sample_i, sequence in enumerate(sequences_positive + sequences_negative):\n",
    "        x[sample_i] = sequence2matrix(sequence, FLAGS.sequence_size, w2v_model)\n",
    "\n",
    "    y_positive = np.ones((len(sequences_positive), 1))\n",
    "    y_negative = np.zeros((len(sequences_negative), 1))\n",
    "    y = np.concatenate((y_positive, y_negative))\n",
    "\n",
    "    # shuffling\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(y.shape[0]))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "    return x_shuffled, y_shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dir = os.path.join(FLAGS.cnn_evaluate)\n",
    "\n",
    "# load cnn classifiers\n",
    "cnn_classifiers = set()\n",
    "for cls_name in os.listdir(FLAGS.cnn_evaluate):\n",
    "    cnn_classifiers.add(cls_name)\n",
    "\n",
    "# load gt, samples, entities\n",
    "data = load_json(FLAGS.output_dir+'column_gt_extend.json')\n",
    "samples = load_json(\n",
    "    FLAGS.output_dir+'sample_classes.json')\n",
    "entities = load_json(\n",
    "    FLAGS.output_dir+'entities_classes.json')\n",
    "\n",
    "\n",
    "def predict(test_x, classifier_name):\n",
    "    # Load the saved model using TensorFlow 2.0\n",
    "    loaded_model = load_model(cnn_dir, classifier_name)\n",
    "\n",
    "    desired_shape = (test_x.shape[0], loaded_model.layers[0].input_shape[1],\n",
    "                     loaded_model.layers[0].input_shape[2], test_x.shape[-1])\n",
    "\n",
    "    test_x_reshaped = test_x[:,:desired_shape[1],:,:]\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = loaded_model.predict(test_x_reshaped)\n",
    "    probabilities = tf.nn.sigmoid(predictions).numpy().flatten()\n",
    "\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "     column 0 predicted\n",
      "1/1 [==============================] - 0s 345ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "     column 5 predicted\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "     column 10 predicted\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "     column 15 predicted\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "     column 20 predicted\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n"
     ]
    }
   ],
   "source": [
    "col_class_p = dict()\n",
    "\n",
    "for col_i, col in enumerate(data.keys()):\n",
    "    cells = list(data[col]['data'].values())[0]\n",
    "\n",
    "    corresponding_ent = []\n",
    "    for cell in cells:\n",
    "        if cell in entities:\n",
    "            corresponding_ent=entities[cell]\n",
    "\n",
    "    if FLAGS.synthetic_column_type >= 0:\n",
    "        if FLAGS.synthetic_column_type > 0:\n",
    "            units = random_cells2synthetic_columns(\n",
    "                cells, FLAGS.synthetic_column_size, FLAGS.synthetic_column_type)\n",
    "        else:\n",
    "            units = ordered_cells2synthetic_columns(\n",
    "                cells, FLAGS.synthetic_column_size)\n",
    "    else:\n",
    "        units = permutation_cells2synthetic_columns(cells)\n",
    "\n",
    "    X = np.zeros((len(units), FLAGS.sequence_size, w2v_model.vector_size, 1))\n",
    "    for i, unit in enumerate(units):\n",
    "        seq = synthetic_columns2sequence(unit, FLAGS.sequence_size)\n",
    "        X[i] = sequence2matrix(seq, FLAGS.sequence_size, w2v_model)\n",
    "\n",
    "    for classifier in corresponding_ent['candidate_classes']:\n",
    "        if classifier in cnn_classifiers:\n",
    "            if len(samples[classifier]['general_pos_samples']) == 0 or len(samples[classifier]['negative_samples'])==0 :\n",
    "                continue\n",
    "            p_ents, n_ents = align_samples(\n",
    "                samples[classifier]['general_pos_samples'], samples[classifier]['negative_samples'])\n",
    "            X, _ = embedding(p_ents, n_ents)\n",
    "            col_class = '%s,%s' % (col, classifier)\n",
    "            p = predict(X, classifier)\n",
    "            score = np.mean(p)\n",
    "            col_class_p[col_class] = score\n",
    "\n",
    "    if col_i % 5 == 0:\n",
    "        print('     column %d predicted' % col_i)\n",
    "\n",
    "col_class_p_serializable = {\n",
    "    key: float(value) if isinstance(value, np.float32) else value\n",
    "    for key, value in col_class_p.items()\n",
    "}\n",
    "\n",
    "out_filename = 'p_%s.json' % os.path.basename(FLAGS.cnn_evaluate)\n",
    "with open(os.path.join(prediction_dir, out_filename), 'w') as fp:\n",
    "    json.dump(col_class_p_serializable, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction by Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_prefix = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "def lookup_wikidata_classes(query, limit=1):\n",
    "    wikidata = WikidataAPI()\n",
    "    entities = wikidata.getKGEntities(query, limit, 'item')\n",
    "    i = 0\n",
    "    classes = list()\n",
    "    entity_classes = dict()\n",
    "    for ent in entities:\n",
    "        classes.append(ent.getId().split(wd_prefix)[1])\n",
    "        i += 1\n",
    "        if len(classes) == 0:\n",
    "            print('Zero classes')\n",
    "        if len(classes) > 0:\n",
    "            entity_classes = dict()\n",
    "            entity_classes = classes\n",
    "    return entity_classes\n",
    "\n",
    "\n",
    "# lookup entities and classes from DBPedia\n",
    "def lookup_resources(cell_text):\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "    dbp_prefix = 'http://dbpedia.org/resource/'\n",
    "    entity_classes = dict()\n",
    "    cell_items = list()\n",
    "    cell_brackets = re.findall('\\((.*?)\\)', cell_text)\n",
    "    for cell_bracket in cell_brackets:\n",
    "        cell_text = cell_text.replace('(%s)' % cell_bracket, '')\n",
    "    cell_text = cell_text.strip()\n",
    "    if len(cell_text) > 2:\n",
    "        cell_items.append(cell_text)\n",
    "    for cell_bracket in cell_brackets:\n",
    "        if len(cell_bracket) > 2:\n",
    "            cell_items.append(cell_bracket.strip())\n",
    "    for cell_item in cell_items:\n",
    "        try:\n",
    "            lookup_url = 'http://lookup.dbpedia.org/api/search/KeywordSearch?MaxHits=2&QueryString=%s' % cell_item\n",
    "            lookup_res = requests.get(lookup_url)\n",
    "            root = ET.fromstring(lookup_res.content)\n",
    "            for child in root:\n",
    "                entity = child[1].text.split(dbp_prefix)[1]\n",
    "                classes = list()\n",
    "                for cc in child[3]:\n",
    "                    cls_URI = cc[1].text\n",
    "                    if dbo_prefix in cls_URI:\n",
    "                        classes.append(cls_URI.split(dbo_prefix)[1])\n",
    "                entity_classes[entity] = classes\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    return entity_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookup-based prediction column by column\n",
      "     column 0 annotated\n",
      "     column 10 annotated\n",
      "     column 20 annotated\n"
     ]
    }
   ],
   "source": [
    "print('Lookup-based prediction column by column')\n",
    "\n",
    "col_class_p = dict()\n",
    "for col_i, col in enumerate(data.keys()):\n",
    "    cells = list(data[col]['data'].values())[0]\n",
    "    cell_classes = dict()\n",
    "    unq_clses = set()\n",
    "    for cell in cells:\n",
    "        classes = lookup_wikidata_classes(cell)\n",
    "        cell_classes[cell] = classes\n",
    "        unq_clses = unq_clses | set(classes)\n",
    "\n",
    "    for cls in unq_clses:\n",
    "        count = 0\n",
    "        for cell in cells:\n",
    "            if cls in cell_classes[cell]:\n",
    "                count += 1\n",
    "        p = float(count) / float(len(cells))\n",
    "        col_class = '%s,%s' % (col, cls)\n",
    "        col_class_p[col_class] = p\n",
    "\n",
    "    if col_i % 10 == 0:\n",
    "        print('     column %d annotated' % col_i)\n",
    "    if (col_i + 1) % 30 == 0:\n",
    "        time.sleep(60*5)\n",
    "\n",
    "out_filename = 'p_lookup.json'\n",
    "with open(os.path.join(prediction_dir, out_filename), 'w') as fp:\n",
    "    json.dump(col_class_p, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction ensemble of results by lookup and cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file containing prediction by CNN training\n",
    "predictions_model = os.path.join(current_path,'output/predictions/p_cnn_1_2_1.00.json')\n",
    "# file containing prediction by lookup\n",
    "predictions_voting = os.path.join(current_path,'output/predictions/p_lookup.json')\n",
    "\n",
    "p_voting = dict()\n",
    "p_voting = load_json(predictions_voting)\n",
    "\n",
    "p_model = dict()\n",
    "p_model = load_json(predictions_model)\n",
    "\n",
    "p = dict()\n",
    "for col_cls in p_voting:\n",
    "    if p_voting[col_cls] >= 0.6:\n",
    "        p[col_cls] = p_voting[col_cls]\n",
    "    elif p_voting[col_cls] >= 0.25:\n",
    "        if col_cls in p_model:\n",
    "            p[col_cls] = p_model[col_cls]\n",
    "        else:\n",
    "            p[col_cls] = p_voting[col_cls]\n",
    "    else:\n",
    "        p[col_cls] = p_voting[col_cls]\n",
    "\n",
    "out_filename = '%s_lookup.json' % os.path.basename(\n",
    "    predictions_model).split('.json')[0]\n",
    "with open(os.path.join(prediction_dir, out_filename), 'w') as fp:\n",
    "    json.dump(p, fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('colnet-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31dfa0067d95d91f2293773f41a1bcba4d2bbc525ad33be42c4f090f17bf1713"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
