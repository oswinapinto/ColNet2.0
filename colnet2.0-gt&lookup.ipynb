{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0 - Ground Truth Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import sparql\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from endpoints import WikidataEndpoint\n",
    "import re\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from lookup import WikidataAPI\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--output_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'output'),\n",
    "    help='Directory of output'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--in_dir',\n",
    "    type=str,\n",
    "    default='../SemTab_DataSets/Round1DataSets/Valid',\n",
    "    help='Input dir containing data tables gt and targets'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--tables',\n",
    "    type=str,\n",
    "    default='/tables',\n",
    "    help='input data tables'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--target',\n",
    "    type=str,\n",
    "    default='/targets/cea_targets.csv',\n",
    "    help='Target file for CEA'\n",
    "\n",
    "    # default='/targets/cta_targets.csv',\n",
    "    # help='Target file for CTA'\n",
    "\n",
    "    # default='/targets/cpa_targets.csv',\n",
    "    # help='Target file for CPA'\n",
    ")\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_prefix = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "def get_wikidata_superclasses(query, attempts = 1):\n",
    "    sparqlw = SPARQLWrapper(\n",
    "            'https://query.wikidata.org/bigdata/namespace/wdq/sparql')\n",
    "    query = \"\"\"\n",
    "            SELECT DISTINCT ?uri\n",
    "            WHERE {\n",
    "                wd:%s wdt:P31/wdt:P270* ?uri.\n",
    "            }\"\"\" % (query)\n",
    "    try:\n",
    "        sparqlw.setQuery(query)\n",
    "        sparqlw.setReturnFormat(JSON)\n",
    "        results = sparqlw.query().convert()\n",
    "\n",
    "        result_set = set()\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            uri_value = result[\"uri\"][\"value\"]\n",
    "            if uri_value.startswith(wd_prefix):\n",
    "                result_set.add(uri_value.split(wd_prefix)[1])\n",
    "        return result_set\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Query '%s' failed. Attempts: %s\" % (query, str(attempts)))\n",
    "        time.sleep(60)  # To avoid rate limits, sleep for 60 seconds\n",
    "        attempts -= 1\n",
    "        if attempts > 0:\n",
    "            return get_wikidata_superclasses(query, attempts)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def get_wikidata_classes(query, limit=1):\n",
    "    wikidata = WikidataAPI()\n",
    "    entities = wikidata.getKGEntities(query, limit, 'item')\n",
    "    i = 0\n",
    "    classes = list()\n",
    "    entity_classes = dict()\n",
    "    for ent in entities:\n",
    "        classes.append(ent.getId().split(wd_prefix)[1])\n",
    "        i += 1\n",
    "        if len(classes) == 0:\n",
    "            print('Zero classes')\n",
    "        if len(classes) > 0:\n",
    "            entity_classes = dict()\n",
    "            entity_classes = classes\n",
    "    return entity_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the target file\n",
    "target_df = pd.read_csv(os.path.join(FLAGS.in_dir+FLAGS.target), header=None, nrows=100)\n",
    "target_df.columns = ['Table_id', 'Row_id', 'Column_id']\n",
    "\n",
    "target_dict = dict()\n",
    "for index, row in target_df.iterrows():\n",
    "    if row['Table_id'] not in target_dict:\n",
    "        target_dict[row['Table_id']] = []\n",
    "    target_dict[row['Table_id']].append(int(row['Column_id']))\n",
    "\n",
    "# reading the input data\n",
    "data = dict()\n",
    "\n",
    "for file in target_dict:\n",
    "    data[file] = dict()\n",
    "    df_data = pd.DataFrame()\n",
    "    df_title = pd.DataFrame()\n",
    "\n",
    "    filename = file + '.csv'\n",
    "    tab_data_file = os.path.join(FLAGS.in_dir+FLAGS.tables, filename)\n",
    "\n",
    "    if len(target_dict[file]) > 0:\n",
    "        df_data = pd.read_csv(tab_data_file, header=None, skiprows=[\n",
    "                              0], usecols=target_dict[file])\n",
    "        df_title = pd.read_csv(tab_data_file, header=None,\n",
    "                               usecols=target_dict[file], nrows=1)\n",
    "    else:\n",
    "        df_data = pd.read_csv(tab_data_file, header=None, skiprows=[0])\n",
    "        df_title = pd.read_csv(tab_data_file, header=None, nrows=1)\n",
    "    # adding the column headers to the data dictionary\n",
    "    try:\n",
    "        data[file]['column_titles'] = list(df_title.iloc[0, :])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    file_element = dict()\n",
    "    for column in df_data.columns:\n",
    "        # without cell value repetition\n",
    "        file_element[column] = list(set(df_data[column]))\n",
    "    data[file]['data'] = file_element\n",
    "\n",
    "    # getting superclasses\n",
    "    for cells in data[file]['data']:\n",
    "        data[file]['gt'] = []\n",
    "        for cell in data[file]['data'][cells]:\n",
    "            wd_class = get_wikidata_classes(cell)\n",
    "            if len(wd_class) > 0:\n",
    "                for cls in wd_class :\n",
    "                    data[file]['gt'].append(cls)\n",
    "                    wd_superclass = get_wikidata_superclasses(cls)\n",
    "                    if wd_superclass and len(wd_superclass) > 0 :\n",
    "                        for item in wd_superclass:\n",
    "                            data[file]['gt'].append(item)\n",
    "                data[file]['gt']=list(set(data[file]['gt']))\n",
    "\n",
    "with open(os.path.join(FLAGS.output_dir, 'column_gt_extend.json'), 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookup and update new entities and classes\n"
     ]
    }
   ],
   "source": [
    "print('''Lookup and update new entities and classes''')\n",
    "ent_cls = dict()\n",
    "i = 0\n",
    "for file in data:\n",
    "    filename = file\n",
    "    for col in data[file]['data']:\n",
    "        column_index = col\n",
    "        for line_j in range(len(data[file]['data'][col])):\n",
    "            i += 1\n",
    "            cell = data[file]['data'][col][line_j]\n",
    "            if isinstance(cell, str):  # Check if cell is a string\n",
    "                cell = cell.replace('[', '').replace(']', '')\n",
    "            if cell not in ent_cls:\n",
    "                ent_cls[cell] = {\n",
    "                    'candidate_classes': get_wikidata_classes(cell, 1)\n",
    "                }\n",
    "    with open('output/entities_classes.json', 'w') as fp:\n",
    "        json.dump(ent_cls, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_dict = dict()\n",
    "\n",
    "for filename in target_dict:\n",
    "    candidate_dict[filename] = dict()\n",
    "    for col_index in target_dict[filename]:\n",
    "        candidate_dict[filename][col_index] = []\n",
    "\n",
    "    for col_index in target_dict[filename]:\n",
    "        for cell in ent_cls:\n",
    "            try:\n",
    "                if cell in data[filename]['data'][0]:\n",
    "                    for candidate_cls in ent_cls[cell]['candidate_classes']:\n",
    "                        for candidate_class in ent_cls[cell]['candidate_classes']:\n",
    "                            if (candidate_class, cell) not in candidate_dict[filename][col_index]:\n",
    "                                candidate_dict[filename][col_index].append(\n",
    "                                    (candidate_class, cell))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# saving the candidate_dict to a JSON file\n",
    "with open('output/candidate_columns.json', 'w') as fp:\n",
    "    json.dump(candidate_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q6386554</td>\n",
       "      <td>Kelso Township</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q6500028</td>\n",
       "      <td>Laurel Township</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q987536</td>\n",
       "      <td>Brookville Township</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q5604743</td>\n",
       "      <td>Greenville Township</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q7998039</td>\n",
       "      <td>Mill Township</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Q796762</td>\n",
       "      <td>Bookends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Q217750</td>\n",
       "      <td>Violetta Villas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Q71278710</td>\n",
       "      <td>Easy Pieces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Q290268</td>\n",
       "      <td>Dirty Deeds Done Dirt Cheap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Q27568</td>\n",
       "      <td>Through the Past, Darkly (Big Hits Vol. 2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                                      entity\n",
       "0    Q6386554                              Kelso Township\n",
       "1    Q6500028                             Laurel Township\n",
       "2     Q987536                         Brookville Township\n",
       "3    Q5604743                         Greenville Township\n",
       "4    Q7998039                               Mill Township\n",
       "..        ...                                         ...\n",
       "64    Q796762                                    Bookends\n",
       "65    Q217750                             Violetta Villas\n",
       "66  Q71278710                                 Easy Pieces\n",
       "67    Q290268                 Dirty Deeds Done Dirt Cheap\n",
       "68     Q27568  Through the Past, Darkly (Big Hits Vol. 2)\n",
       "\n",
       "[69 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entities = pd.DataFrame()\n",
    "\n",
    "for filename in candidate_dict:\n",
    "    for col in candidate_dict[filename]:\n",
    "        df_entities = df_entities._append(pd.DataFrame(\n",
    "            candidate_dict[filename][col], columns=['type', 'entity']), ignore_index=True)\n",
    "\n",
    "\n",
    "df_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes_list = list()\n",
    "for cell in ent_cls:\n",
    "    classes_list.append(ent_cls[cell]['candidate_classes'])\n",
    "\n",
    "# getting unique tuples\n",
    "unique_classes = set(tuple(row) for row in classes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_classes(cand_dict):\n",
    "    candidate_classes = {}\n",
    "\n",
    "    for file, columns in cand_dict.items():\n",
    "        for col, cell_data in columns.items():\n",
    "            neighbouring_classes = set()\n",
    "\n",
    "            for cell in cell_data:\n",
    "                neighbouring_classes.add(cell[0])  # add the class to neighbouring_classes\n",
    "\n",
    "                # initialize candidate_classes entry if it doesn't exist\n",
    "                if cell[0] not in candidate_classes:\n",
    "                    candidate_classes[cell[0]] = {\n",
    "                        'coexist_cls': set(),\n",
    "                        'positive_samples': set(),\n",
    "                        'negative_samples': set(),\n",
    "                        'general_pos_samples': set()\n",
    "                    }\n",
    "\n",
    "            # updating co-existing classes for each class in neighbouring_classes\n",
    "            for candidate_class in neighbouring_classes:\n",
    "                other_classes = neighbouring_classes - {candidate_class}\n",
    "                candidate_classes[candidate_class]['coexist_cls'].update(other_classes)\n",
    "\n",
    "    return candidate_classes\n",
    "\n",
    "def get_entities(cls, limit=1):\n",
    "    wikidata = WikidataAPI()\n",
    "    classes = list()\n",
    "    try:\n",
    "        wd_result = wikidata.getKGEntities(cls, limit, \"item\")\n",
    "        for entity in wd_result:\n",
    "            classes.append(entity.getId().split(wd_prefix)[1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return classes\n",
    "    \n",
    "def process_candidate_classes(sample_classes, df_entities):\n",
    "    for cand_class in sample_classes:\n",
    "        sample_classes[cand_class]['positive_samples'].update(\n",
    "            set(df_entities[df_entities.type == cand_class].entity)) # updating pos samples\n",
    "        for cls_set in unique_classes:\n",
    "            if cand_class in cls_set:\n",
    "                sample_classes[cand_class]['coexist_cls'] = sample_classes[cand_class]['coexist_cls'] - set(cls_set) # remove coexisting classes for same entity\n",
    "\n",
    "        for neighbour_cls in sample_classes[cand_class]['coexist_cls']:\n",
    "            sample_classes[cand_class]['negative_samples'].update(\n",
    "                set(df_entities[df_entities.type == neighbour_cls].entity)) # updating neg samples\n",
    "\n",
    "        sample_classes[cand_class]['general_pos_samples'] = set(\n",
    "            get_entities(cand_class, 1)) # updating general pos samples\n",
    "\n",
    "        sample_classes[cand_class]['coexist_cls'] = list(\n",
    "            sample_classes[cand_class]['coexist_cls'])\n",
    "        sample_classes[cand_class]['positive_samples'] = list(\n",
    "            sample_classes[cand_class]['positive_samples'])\n",
    "        sample_classes[cand_class]['negative_samples'] = list(\n",
    "            sample_classes[cand_class]['negative_samples'])\n",
    "        sample_classes[cand_class]['general_pos_samples'] = list(\n",
    "            sample_classes[cand_class]['general_pos_samples'])\n",
    "\n",
    "    return sample_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise sample classes\n",
    "sample_classes = get_candidate_classes(candidate_dict)\n",
    "\n",
    "# printing information about the classes and the number of neighbours\n",
    "# for key, value in sample_classes.items():\n",
    "#     print(f\"Class: {key} with {len(value['coexist_cls'])} neighbouring classes\")\n",
    "\n",
    "# processing candidate classes and updating samples\n",
    "sample_classes = process_candidate_classes(sample_classes, df_entities)\n",
    "\n",
    "# saving the processed sample classes\n",
    "with open('output/sample_classes.json', 'w') as fp:\n",
    "    json.dump(sample_classes, fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('colnet-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31dfa0067d95d91f2293773f41a1bcba4d2bbc525ad33be42c4f090f17bf1713"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
